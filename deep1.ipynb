{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.6.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.22.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c10dhZtFIf_j",
        "colab_type": "code",
        "execution": {
          "iopub.status.busy": "2020-09-16T12:06:52.753Z",
          "iopub.execute_input": "2020-09-16T12:06:52.762Z",
          "shell.execute_reply": "2020-09-16T12:06:57.181Z",
          "iopub.status.idle": "2020-09-16T12:06:57.190Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8ae6d8b3-a315-4483-b014-5959a90f2acf"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets.samples_generator import make_regression \n",
        "from scipy import stats \n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ElZmwf0If_s",
        "colab_type": "text"
      },
      "source": [
        "# 1. Gradient descend: 1-D\n",
        "\n",
        "This is the simplest implementation of the derivative:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bP9oaJgIf_s",
        "colab_type": "code",
        "execution": {
          "iopub.status.busy": "2020-09-16T12:07:04.284Z",
          "iopub.execute_input": "2020-09-16T12:07:04.293Z",
          "iopub.status.idle": "2020-09-16T12:07:04.313Z",
          "shell.execute_reply": "2020-09-16T12:07:04.324Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96b0fd37-8358-4d54-c4a7-817866157521"
      },
      "source": [
        "# numerical derivative at a point x\n",
        "\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "def fin_dif(x, \n",
        "            f, \n",
        "            h = 0.00001):\n",
        "    '''\n",
        "    This method returns the derivative of f at x\n",
        "    by using the finite difference method\n",
        "    '''\n",
        "    return (f(x+h) - f(x))/h\n",
        "\n",
        "x = 2.0\n",
        "print(\"{:2.4f}\".format(fin_dif(x,f)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d_y8uEaBr-g",
        "colab_type": "text"
      },
      "source": [
        "We can apply this definition to find the minimum of a function. Alternatively, if we know the analytical derivative of a function, we can also use it.\n",
        "\n",
        "In the next example, we apply the analytical derivative but we do not get the minimum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMnzvWRnIgAO",
        "colab_type": "code",
        "execution": {
          "iopub.status.busy": "2020-09-16T12:08:14.840Z",
          "iopub.execute_input": "2020-09-16T12:08:14.847Z",
          "iopub.status.idle": "2020-09-16T12:08:14.965Z",
          "shell.execute_reply": "2020-09-16T12:08:15.247Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "b902de88-31e3-43e7-bff1-a364f108e009"
      },
      "source": [
        "x = np.linspace(-10,20,100)\n",
        "y = x**2 - 6*x + 5\n",
        "start = 15\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "fig.set_facecolor('#EAEAF2')\n",
        "plt.plot(x,y, 'r-')\n",
        "plt.plot([start],[start**2 - 6*start + 5],'o')\n",
        "ax.text(start,\n",
        "        start**2 - 6*start + 35,\n",
        "        'Start',\n",
        "        ha='center',\n",
        "        color=sns.xkcd_rgb['blue'],\n",
        "       )\n",
        "\n",
        "d = 2 * start - 6\n",
        "end = start - d\n",
        "\n",
        "plt.plot([end],[end**2 - 6*end + 5],'o')\n",
        "plt.ylim([-10,250])\n",
        "plt.gcf().set_size_inches((10,3))\n",
        "plt.grid(True)\n",
        "ax.text(end,\n",
        "        start**2 - 6*start + 35,\n",
        "        'End',\n",
        "        ha='center',\n",
        "        color=sns.xkcd_rgb['green'],\n",
        "       )\n",
        "plt.show"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYjGCRfECgDb",
        "colab_type": "text"
      },
      "source": [
        "Do you know why?\n",
        "\n",
        "Complete the follwing code to find the minimum of a function.\n",
        "\n",
        "Hints: \n",
        "+ You have to define a point where to start the search. \n",
        "+ You have to define a step size to move. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj5ZFZJIIgAS",
        "colab_type": "code",
        "execution": {
          "iopub.status.busy": "2020-09-16T12:08:22.473Z",
          "iopub.execute_input": "2020-09-16T12:08:22.480Z",
          "iopub.status.idle": "2020-09-16T12:08:22.494Z",
          "shell.execute_reply": "2020-09-16T12:08:22.500Z"
        },
        "colab": {}
      },
      "source": [
        "old_min = 0\n",
        "temp_min = 15\n",
        "step_size = 0.01\n",
        "precision = 0.0001\n",
        "\n",
        "def f(x):\n",
        "    return x**2 - 6*x + 5\n",
        "    \n",
        "def f_derivative(x):\n",
        "    import math\n",
        "    return 2*x -6\n",
        "\n",
        "mins = []\n",
        "cost = []\n",
        "\n",
        "while abs(temp_min - old_min) > precision:\n",
        "    # your code here\n",
        "\n",
        "# rounding the result to 2 digits because of the step size\n",
        "print(\"Local minimum occurs at {:3.6f}.\".format(round(temp_min,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1Vsy0aDA7e",
        "colab_type": "text"
      },
      "source": [
        "If it works, you can visualize the evolution of the value of the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7B2RQEVIgAZ",
        "colab_type": "code",
        "execution": {
          "iopub.status.busy": "2020-09-16T12:08:32.911Z",
          "iopub.execute_input": "2020-09-16T12:08:32.917Z",
          "iopub.status.idle": "2020-09-16T12:08:33.025Z",
          "shell.execute_reply": "2020-09-16T12:08:33.046Z"
        },
        "colab": {},
        "outputId": "86823957-7775-4b0b-9be1-089cef9288f4"
      },
      "source": [
        "x = np.linspace(-10,20,100)\n",
        "y = x**2 - 6*x + 5\n",
        "\n",
        "x, y = (zip(*enumerate(cost)))\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "fig.set_facecolor('#EAEAF2')\n",
        "plt.plot(x,y, 'r-', alpha=0.7)\n",
        "plt.ylim([-10,150])\n",
        "plt.gcf().set_size_inches((10,3))\n",
        "plt.grid(True)\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntkp6D35DPpO",
        "colab_type": "text"
      },
      "source": [
        "You can also visualize the function points you have explored:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzeVtZ0uIgAe",
        "colab_type": "code",
        "execution": {
          "iopub.status.busy": "2020-09-16T12:08:36.119Z",
          "iopub.execute_input": "2020-09-16T12:08:36.126Z",
          "iopub.status.idle": "2020-09-16T12:08:36.241Z",
          "shell.execute_reply": "2020-09-16T12:08:36.261Z"
        },
        "colab": {},
        "outputId": "0512661d-a331-4d6b-cf88-ab38df591209"
      },
      "source": [
        "x = np.linspace(-10,20,100)\n",
        "y = x**2 - 6*x + 5\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "fig.set_facecolor('#EAEAF2')\n",
        "plt.plot(x,y, 'r-')\n",
        "plt.ylim([-10,250])\n",
        "plt.gcf().set_size_inches((10,3))\n",
        "plt.grid(True)\n",
        "plt.plot(mins,cost,'o', alpha=0.3)\n",
        "ax.text(start,\n",
        "        start**2 - 6*start + 25,\n",
        "        'Start',\n",
        "        ha='center',\n",
        "        color=sns.xkcd_rgb['blue'],\n",
        "       )\n",
        "ax.text(mins[-1],\n",
        "        cost[-1]+20,\n",
        "        'End (%s steps)' % len(mins),\n",
        "        ha='center',\n",
        "        color=sns.xkcd_rgb['blue'],\n",
        "       )\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5M-fnEyIgAj",
        "colab_type": "text"
      },
      "source": [
        "## 2. From derivatives to gradient: $n$-dimensional function minimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NOS5N-AIgAl",
        "colab_type": "code",
        "execution": {
          "iopub.status.busy": "2020-09-16T12:09:26.323Z",
          "iopub.execute_input": "2020-09-16T12:09:26.329Z",
          "iopub.status.idle": "2020-09-16T12:09:26.347Z",
          "shell.execute_reply": "2020-09-16T12:09:26.366Z"
        },
        "colab": {},
        "outputId": "1ef86aa0-7163-4f10-d0e6-5a68a07e5ab2"
      },
      "source": [
        "def f(x):\n",
        "    return sum(x_i**2 for x_i in x)\n",
        "\n",
        "def fin_dif_partial_centered(x, \n",
        "                             f, \n",
        "                             i, \n",
        "                             h=1e-6):\n",
        "    '''\n",
        "    This method returns the partial derivative of the i-th component of f at x\n",
        "    by using the centered finite difference method\n",
        "    '''\n",
        "    w1 = [x_j + (h if j==i else 0) for j, x_j in enumerate(x)]\n",
        "    w2 = [x_j - (h if j==i else 0) for j, x_j in enumerate(x)]\n",
        "    return (f(w1) - f(w2))/(2*h)\n",
        "\n",
        "def fin_dif_partial_old(x, \n",
        "                        f, \n",
        "                        i, \n",
        "                        h=1e-6):\n",
        "    '''\n",
        "    This method returns the partial derivative of the i-th component of f at x\n",
        "    by using the (non-centered) finite difference method\n",
        "    '''\n",
        "    w1 = [x_j + (h if j==i else 0) for j, x_j in enumerate(x)]\n",
        "    return (f(w1) - f(x))/h\n",
        "\n",
        "def gradient_centered(x, \n",
        "                      f, \n",
        "                      h=1e-6):\n",
        "    '''\n",
        "    This method returns the gradient vector of f at x\n",
        "    by using the centered finite difference method\n",
        "    '''\n",
        "    return[round(fin_dif_partial_centered(x,f,i,h), 10) for i,_ in enumerate(x)]\n",
        "\n",
        "def gradient_old(x, \n",
        "                 f, \n",
        "                 h=1e-6):\n",
        "    '''\n",
        "    This method returns the the gradient vector of f at x\n",
        "    by using the (non-centered)ç finite difference method\n",
        "    '''\n",
        "    return[round(fin_dif_partial_old(x,f,i,h), 10) for i,_ in enumerate(x)]\n",
        "\n",
        "x = [1.0,1.0,1.0]\n",
        "\n",
        "print('{:.6f}'.format(f(x)), gradient_centered(x,f))\n",
        "print('{:.6f}'.format(f(x)), gradient_old(x,f))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.000000 [2.0000000001, 2.0000000001, 2.0000000001]\n",
            "3.000000 [2.0000009999, 2.0000009999, 2.0000009999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJnaRjh7IgAr",
        "colab_type": "text"
      },
      "source": [
        "The function we have evaluated, $f({\\mathbf x}) = x_1^2+x_2^2+x_3^2$, is $3$ at $(1,1,1)$ and the gradient vector at this point is $(2,2,2)$. \n",
        "\n",
        "Then, we can follow this steps to maximize (or minimize) the function:\n",
        "\n",
        "+ Start from a random $\\mathbf{x}$ vector.\n",
        "+ Compute the gradient vector.\n",
        "+ Walk a small step in the opposite direction of the gradient vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ntshMuIgAt",
        "colab_type": "text"
      },
      "source": [
        "> It is important to be aware that this gradient computation is very expensive: if $\\mathbf{x}$ has dimension $n$, we have to evaluate $f$ at $2*n$ points.\n",
        "\n",
        "\n",
        "### How to use the gradient.\n",
        "\n",
        "$f(x) = \\sum_i x_i^2$, takes its mimimum value when all $x$ are 0. \n",
        "\n",
        "Let's check it for $n=3$: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3apmZdxCIgAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euc_dist(v1,v2):\n",
        "    import numpy as np\n",
        "    import math\n",
        "    v = np.array(v1)-np.array(v2)\n",
        "    return math.sqrt(sum(v_i ** 2 for v_i in v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Z_DkqcIgAw",
        "colab_type": "text"
      },
      "source": [
        "Let's start by choosing a random vector and then walking a step in the opposite direction of the gradient vector. We will stop when the difference between the new solution and the old solution is less than a tolerance value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoCNttw4IgAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choosing a random vector\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "x = [random.randint(-10,10) for i in range(3)]\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH6CunsyIgAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step(x,\n",
        "         grad,\n",
        "         alpha):\n",
        "    '''\n",
        "    This function makes a step in the opposite direction of the gradient vector \n",
        "    in order to compute a new value for the target function.\n",
        "    '''\n",
        "    return [x_i - alpha * grad_i for x_i, grad_i in zip(x,grad)]\n",
        "\n",
        "tol = 1e-15\n",
        "alpha = 0.01\n",
        "while True:\n",
        "    grad = gradient_centered(x,f)\n",
        "    next_x = step(x,grad,alpha)\n",
        "    if euc_dist(next_x,x) < tol:\n",
        "        break\n",
        "    x = next_x\n",
        "print([round(i,10) for i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDNwLYGxIgA2",
        "colab_type": "text"
      },
      "source": [
        "### Alpha\n",
        "\n",
        "The step size, **alpha**, is a slippy concept: if it is too small we will slowly converge to the solution, if it is too large we can diverge from the solution. \n",
        "\n",
        "There are several policies to follow when selecting the step size:\n",
        "\n",
        "+ Constant size steps. In this case, the size step determines the precision of the solution.\n",
        "+ Decreasing step sizes.\n",
        "+ At each step, select the optimal step.\n",
        "\n",
        "The last policy is good, but too expensive. In this case we would consider a fixed set of values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEIgk7tgIgA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "step_size = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63eTMIrIgA5",
        "colab_type": "text"
      },
      "source": [
        "## Learning from data\n",
        "\n",
        "In general, we have:\n",
        "\n",
        "+ A dataset $(\\mathbf{x},y)$ of $n$ examples. \n",
        "+ A target function $f_\\mathbf{w}$, that we want to minimize, representing the **discrepancy between our data and the model** we want to fit. The model is represented by a set of parameters $\\mathbf{w}$. \n",
        "+ The gradient of the target function, $g_f$. \n",
        "\n",
        "\n",
        "In the most common case $f$ represents the errors from a data representation model $M$. To fit the model is to find the optimal parameters $\\mathbf{w}$ that minimize the following expression:\n",
        "\n",
        "$$ f_\\mathbf{w} = \\frac{1}{n} \\sum_{i} (y_i - M(\\mathbf{x}_i,\\mathbf{w}))^2 $$\n",
        "\n",
        "For example, $(\\mathbf{x},y)$ can represent:\n",
        "\n",
        "+ $\\mathbf{x}$: the behavior of a \"Candy Crush\" player; $y$: monthly payments. \n",
        "+ $\\mathbf{x}$: sensor data about your car engine; $y$: probability of engine error.\n",
        "+ $\\mathbf{x}$: finantial data of a bank customer; $y$: customer rating.\n",
        "\n",
        "> If $y$ is a real value, it is called a *regression* problem.\n",
        "\n",
        "> If $y$ is binary/categorical, it is called a *classification* problem. \n",
        "\n",
        "Let's suppose that our model is a one-dimensional linear model $M(\\mathbf{x},\\mathbf{w}) = w \\cdot x $. \n",
        "\n",
        "### Batch gradient descend\n",
        "\n",
        "We can implement **gradient descend** in the following way (*batch gradient descend*):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FotL8puIgA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# f = 2x\n",
        "x = np.arange(10)\n",
        "y = np.array([2*i for i in x])\n",
        "\n",
        "# f_target = 1/n Sum (y - wx)**2\n",
        "def target_f(x,y,w):\n",
        "    return np.sum((y - x * w)**2.0) / x.size\n",
        "\n",
        "# gradient_f = 2/n Sum 2wx**2 - 2xy\n",
        "def gradient_f(x,y,w):\n",
        "    return 2 * np.sum(2*w*(x**2) - 2*x*y) / x.size\n",
        "\n",
        "def step(w,grad,alpha):\n",
        "    return w - alpha * grad\n",
        "\n",
        "def BGD_multi_step(target_f, \n",
        "                   gradient_f, \n",
        "                   x, \n",
        "                   y, \n",
        "                   toler = 1e-6):\n",
        "    '''\n",
        "    Batch gradient descend by using a multi-step approach\n",
        "    '''\n",
        "    alphas = [100, 10, 1, 0.1, 0.001, 0.00001]\n",
        "    w = random.random()\n",
        "    val = target_f(x,y,w)\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        gradient = gradient_f(x,y,w)\n",
        "        next_ws = [step(w, gradient, alpha) for alpha in alphas]\n",
        "        next_vals = [target_f(x,y,w) for w in next_ws]\n",
        "        min_val = min(next_vals)\n",
        "        next_w = next_ws[next_vals.index(min_val)]   \n",
        "        next_val = target_f(x,y,next_w)    \n",
        "        if (abs(val - next_val) < toler):\n",
        "            return w\n",
        "        else:\n",
        "            w, val = next_w, next_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5sm2HTBIgA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('{:.6f}'.format(BGD_multi_step(target_f, gradient_f, x, y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b5LXjVwIgBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit\n",
        "BGD_multi_step(target_f, gradient_f, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHkG7FYsIgBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BGD(target_f, \n",
        "        gradient_f, \n",
        "        x, \n",
        "        y, \n",
        "        toler = 1e-6, \n",
        "        alpha=0.01):\n",
        "    '''\n",
        "    Batch gradient descend by using a given step\n",
        "    '''\n",
        "    w = random.random()\n",
        "    val = target_f(x,y,w)\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        gradient = gradient_f(x,y,w)\n",
        "        next_w = step(w, gradient, alpha)\n",
        "        next_val = target_f(x,y,next_w)    \n",
        "        if (abs(val - next_val) < toler):\n",
        "            return w\n",
        "        else:\n",
        "            w, val = next_w, next_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHGUvfYQIgBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('{:.6f}'.format(BGD(target_f, gradient_f, x, y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUlulJROIgBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit\n",
        "BGD(target_f, gradient_f, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k0eRu0YIgBL",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic Gradient Descend\n",
        "\n",
        "The last function evals the whole dataset $(\\mathbf{x}_i,y_i)$ at every step. \n",
        "\n",
        "If the dataset is large, this strategy is too costly. In this case we will use a strategy called **SGD** (*Stochastic Gradient Descend*).\n",
        "\n",
        "When learning from data, the cost function is additive: it is computed by adding sample reconstruction errors. \n",
        "\n",
        "Then, we can compute the estimate the gradient (and move towards the minimum) by using only **one data sample** (or a small data sample).\n",
        "\n",
        "Thus, we will find the minimum by iterating this gradient estimation over the dataset.\n",
        "\n",
        "A full iteration over the dataset is called **epoch**. During an epoch, data must be used in a random order.\n",
        "\n",
        "If we apply this method we have some theoretical guarantees to find a good minimum:\n",
        "+ SGD essentially uses the inaccurate gradient per iteration. Since there is no free food, what is the cost by using approximate gradient? The answer is that the convergence rate is slower than the gradient descent algorithm.\n",
        "+ The convergence of SGD has been analyzed using the theories of convex minimization and of stochastic approximation: it converges almost surely to a global minimum when the objective function is convex or pseudoconvex, and otherwise converges almost surely to a local minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaGt-MziIgBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x = np.arange(10)\n",
        "y = np.array([2*i for i in x])\n",
        "data = list(zip(x,y))\n",
        "\n",
        "for (x_i,y_i) in data:\n",
        "    print('{:3d} {:3d}'.format(x_i,y_i))\n",
        "print(\"\")\n",
        "\n",
        "def in_random_order(data):\n",
        "    '''\n",
        "    Random data generator\n",
        "    '''\n",
        "    import random\n",
        "    indexes = [i for i,_ in enumerate(data)]\n",
        "    random.shuffle(indexes)\n",
        "    for i in indexes:\n",
        "        yield data[i]\n",
        "        \n",
        "for (x_i,y_i) in list(in_random_order(data)):\n",
        "    print('{:3d} {:3d}'.format(x_i,y_i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC2gjQsWIgBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def SGD(target_f, \n",
        "        gradient_f, \n",
        "        x, \n",
        "        y, \n",
        "        toler = 1e-6, \n",
        "        epochs=100, \n",
        "        alpha_0=0.01):\n",
        "    '''\n",
        "    Stochastic gradient descend with automatic step adaptation (by\n",
        "    reducing the step to its 95% when there are iterations with no increase)\n",
        "    '''\n",
        "    data = list(zip(x,y))\n",
        "    w = random.random()\n",
        "    alpha = alpha_0\n",
        "    min_w, min_val = float('inf'), float('inf')\n",
        "    epoch = 0\n",
        "    iteration_no_increase = 0\n",
        "    while epoch < epochs and iteration_no_increase < 100:\n",
        "        val = target_f(x, y, w)\n",
        "        if min_val - val > toler:\n",
        "            min_w, min_val = w, val\n",
        "            alpha = alpha_0\n",
        "            iteration_no_increase = 0\n",
        "        else:\n",
        "            iteration_no_increase += 1\n",
        "            alpha *= 0.95\n",
        "        for x_i, y_i in list(in_random_order(data)):\n",
        "            gradient_i = gradient_f(x_i, y_i, w)\n",
        "            w = w - (alpha *  gradient_i)\n",
        "        epoch += 1\n",
        "    return min_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYgGfjrgIgBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('w: {:.6f}'.format(SGD(target_f, gradient_f, x, y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibQJU8L3IgBW",
        "colab_type": "text"
      },
      "source": [
        "## Exercise: Stochastic Gradient Descent and Linear Regression\n",
        "\n",
        "The linear regression model assumes a linear relationship between data:\n",
        "\n",
        "$$ y_i = w_1 x_i + w_0 $$\n",
        "\n",
        "Let's generate a more realistic dataset (with noise), where $w_1 = 2$ and $w_0 = 0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ar_ElUyIgBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets.samples_generator import make_regression \n",
        "from scipy import stats \n",
        "import random\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmJ9vPATIgBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x: input data\n",
        "# y: noisy output data\n",
        "\n",
        "x = np.random.uniform(0,1,20)\n",
        "\n",
        "# f = 2x + 0\n",
        "def f(x): return 2*x + 0\n",
        "\n",
        "noise_variance =0.1\n",
        "noise = np.random.randn(x.shape[0])*noise_variance\n",
        "y = f(x) + noise\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "fig.set_facecolor('#EAEAF2')\n",
        "plt.xlabel('$x$', fontsize=15)\n",
        "plt.ylabel('$f(x)$', fontsize=15)\n",
        "plt.plot(x, y, 'o', label='y')\n",
        "plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)')\n",
        "plt.ylim([0,2])\n",
        "plt.gcf().set_size_inches((10,3))\n",
        "plt.grid(True)\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXqp3C6FIgBf",
        "colab_type": "text"
      },
      "source": [
        "Complete the following code in order to:\n",
        "+ Compute the value of $w$ by using a estimator based on minimizing the squared error.\n",
        "+ Get from SGD function a vector, `target_value`, representing the value of the target function at each iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctfo30pKIgBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your target function as f_target 1/n Sum (y - wx)**2\n",
        "def target_f(x,y,w):\n",
        "    # your code here\n",
        "    pass\n",
        "  \n",
        "# Write your gradient function\n",
        "def gradient_f(x,y,w):\n",
        "    # your code here\n",
        "    pass\n",
        "\n",
        "def in_random_order(data):\n",
        "    '''\n",
        "    Random data generator\n",
        "    '''\n",
        "    import random\n",
        "    indexes = [i for i,_ in enumerate(data)]\n",
        "    random.shuffle(indexes)\n",
        "    for i in indexes:\n",
        "        yield data[i]\n",
        "\n",
        "# Modify the SGD function to return a 'target_value' vector\n",
        "def SGD(target_f, \n",
        "        gradient_f, \n",
        "        x, \n",
        "        y, \n",
        "        toler = 1e-6, \n",
        "        epochs=100, \n",
        "        alpha_0=0.01):\n",
        "    \n",
        "    # Insert your code among the following lines\n",
        "    \n",
        "    data = zip(x,y)\n",
        "    w = random.random()\n",
        "    alpha = alpha_0\n",
        "    min_w, min_val = float('inf'), float('inf')\n",
        "    iteration_no_increase = 0\n",
        "    epoch = 0\n",
        "    while epoch < epochs and iteration_no_increase < 100:\n",
        "        val = target_f(x, y, w)\n",
        "        if min_val - val > toler:\n",
        "            min_w, min_val = w, val\n",
        "            alpha = alpha_0\n",
        "            iteration_no_increase = 0\n",
        "        else:\n",
        "            iteration_no_increase += 1\n",
        "            alpha *= 0.95\n",
        "        for x_i, y_i in in_random_order(data):\n",
        "            gradient_i = gradient_f(x_i, y_i, w)\n",
        "            w = w - (alpha *  gradient_i)\n",
        "        epoch += 1\n",
        "    return min_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8mKLVp2IgBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the value of the solution\n",
        "\n",
        "w, target_value = SGD(target_f, gradient_f, x, y)\n",
        "print('w: {:.6f}'.format(w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OWmxn-ZIgBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the solution regression line\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "fig.set_facecolor('#EAEAF2')\n",
        "plt.plot(x, y, 'o', label='t')\n",
        "plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)', alpha=0.5)\n",
        "plt.plot([0, 1], [0*w, 1*w], 'r-', label='fitted line', alpha=0.5, linestyle='--')\n",
        "plt.xlabel('input x')\n",
        "plt.ylabel('target t')\n",
        "plt.title('input vs. target')\n",
        "plt.ylim([0,2])\n",
        "plt.gcf().set_size_inches((10,3))\n",
        "plt.grid(True)\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lwSFTf_PIgBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the evolution of the target function value during iterations.\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "fig.set_facecolor('#EAEAF2')\n",
        "plt.plot(np.arange(target_value.size), target_value, 'o', alpha = 0.2)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "plt.grid()\n",
        "plt.gcf().set_size_inches((10,3))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ8noU6NIgBs",
        "colab_type": "text"
      },
      "source": [
        "## Mini-batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFqg_w1jIgBu",
        "colab_type": "text"
      },
      "source": [
        "In code, general batch gradient descent looks something like this:\n",
        "\n",
        "```python\n",
        "nb_epochs = 100\n",
        "for i in range(nb_epochs):\n",
        "    grad = evaluate_gradient(target_f, data, w)\n",
        "    w = w - learning_rate * grad\n",
        "```\n",
        "\n",
        "For a pre-defined number of epochs, we first compute the gradient vector of the target function for the whole dataset w.r.t. our parameter vector. \n",
        "\n",
        "**Stochastic gradient descent** (SGD) in contrast performs a parameter update for each training example and label:\n",
        "\n",
        "```python\n",
        "nb_epochs = 100\n",
        "for i in range(nb_epochs):\n",
        "    np.random.shuffle(data)\n",
        "    for sample in data:\n",
        "        grad = evaluate_gradient(target_f, sample, w)\n",
        "        w = w - learning_rate * grad\n",
        "```\n",
        "\n",
        "Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of $n$ training examples:\n",
        "\n",
        "```python\n",
        "nb_epochs = 100\n",
        "for i in range(nb_epochs):\n",
        "  np.random.shuffle(data)\n",
        "  for batch in get_batches(data, batch_size=50):\n",
        "    grad = evaluate_gradient(target_f, batch, w)\n",
        "    w = w - learning_rate * grad\n",
        "```\n",
        "\n",
        "Minibatch SGD has the advantage that it works with a slightly less noisy estimate of the gradient. However, as the minibatch size increases, the number of updates done per computation done decreases (eventually it becomes very inefficient, like batch gradient descent). \n",
        "\n",
        "There is an optimal trade-off (in terms of computational efficiency) that may vary depending on the data distribution and the particulars of the class of function considered, as well as how computations are implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdAEXJcEIgBv",
        "colab_type": "text"
      },
      "source": [
        "## Loss Funtions\n",
        "\n",
        "Loss functions $L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i \\ell(y_i, f(\\mathbf{x_i}))$ represent the price paid for inaccuracy of predictions in classification/regression problems.\n",
        "\n",
        "In classification this function is often the **zero-one loss**, that is, $ \\ell(y_i, f(\\mathbf{x_i}))$ is zero when $y_i = f(\\mathbf{x}_i)$ and one otherwise.\n",
        "\n",
        "This function is discontinuous with flat regions and is thus extremely hard to optimize using gradient-based methods. For this reason it is usual to consider a proxy to the loss called a *surrogate loss function*. For computational reasons this is usually convex function. Here we have some examples:\n",
        "\n",
        "### Square / Euclidean Loss\n",
        "\n",
        "In regression problems, the most common loss function is the square loss function:\n",
        "\n",
        "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i (y_i - f(\\mathbf{x}_i))^2  $$\n",
        "\n",
        "The square loss function can be re-written and utilized for classification:\n",
        "\n",
        "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i (1 - y_i f(\\mathbf{x}_i))^2  $$\n",
        "\n",
        "\n",
        "### Hinge / Margin Loss (i.e. Suport Vector Machines)\n",
        "\n",
        "The hinge loss function is defined as:\n",
        "\n",
        "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i \\mbox{max}(0, 1 - y_i f(\\mathbf{x}_i))  $$\n",
        "\n",
        "The hinge loss provides a relatively tight, convex upper bound on the 0–1 Loss.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/loss_functions.png?raw=1\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5TbAOhIIgBw",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Loss (Logistic Regression)\n",
        "\n",
        "This function displays a similar convergence rate to the hinge loss function, and since it is continuous, simple gradient descent methods can be utilized. \n",
        "\n",
        "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} log(1 + exp(-y_i f(\\mathbf{x}_i))) $$\n",
        "\n",
        "\n",
        "### Sigmoid Cross-Entropy Loss (Softmax classifier)\n",
        "\n",
        "Cross-Entropy is a loss function that is very used for training **multiclass problems**. We'll focus on models that assume that classes are mutually exclusive. \n",
        "\n",
        "In this case, our labels have this form $\\mathbf{y}_i =(1.0,0.0,0.0)$. If our model predicts a different distribution, say  $ f(\\mathbf{x}_i)=(0.4,0.1,0.5)$, then we'd like to nudge the parameters so that $f(\\mathbf{x}_i)$ gets closer to $\\mathbf{y}_i$.\n",
        "\n",
        "C.Shannon showed that if you want to send a series of messages composed of symbols from an alphabet with distribution $y$ ($y_j$  is the probability of the  $j$-th symbol), then to use the smallest number of bits on average, you should assign  $\\log(\\frac{1}{y_j})$  bits to the  $j$-th symbol. \n",
        "\n",
        "The optimal number of bits is known as **entropy**:\n",
        "\n",
        "$$ H(\\mathbf{y}) = \\sum_j y_j \\log\\frac{1}{y_j} = - \\sum_j y_j \\log y_j$$\n",
        "\n",
        "**Cross entropy** is the number of bits we'll need if we encode symbols by using a wrong distribution $\\hat y$:\n",
        "\n",
        "$$ H(y, \\hat y) =   - \\sum_j y_j \\log \\hat y_j $$ \n",
        "\n",
        "In our case, the real distribution is $\\mathbf{y}$ and the \"wrong\" one is $f(\\mathbf{x}_i)$. So, minimizing **cross entropy** with respect our model parameters will result in the model that best approximates our labels if considered as a probabilistic distribution. \n",
        "\n",
        "Cross entropy is used in combination with **Softmax** classifier. In order to classify $\\mathbf{x}_i$ we could take the index corresponding to the max value of $f(\\mathbf{x}_i)$, but Softmax gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation:\n",
        "\n",
        "$$ P(\\mathbf{y}_i = j \\mid \\mathbf{x_i}) = - log \\left( \\frac{e^{f_j(\\mathbf{x_i})}}{\\sum_k e^{f_k(\\mathbf{x_i})} } \\right) $$\n",
        "\n",
        "where $f_k$ is a linear classifier. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRbnJe1aIgBw",
        "colab_type": "text"
      },
      "source": [
        "## Advanced gradient descend\n",
        "\n",
        "\n",
        "### Momentum\n",
        "\n",
        "SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.\n",
        "\n",
        "<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/ridge2.png?raw=1\">\n",
        "\n",
        "Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction of the update vector of the past time step to the current update vector:\n",
        "\n",
        "$$ v_t = m v_{t-1} + \\alpha \\nabla_w f $$\n",
        "$$ w = w - v_t    $$\n",
        "\n",
        "The momentum $m$ is commonly set to $0.9$.\n",
        "\n",
        "### Nesterov\n",
        "\n",
        "However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.\n",
        "\n",
        "Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term $m v_{t-1}$ to move the parameters $w$. Computing \n",
        "$w - m v_{t-1}$ thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters $w$ but w.r.t. the approximate future position of our parameters:\n",
        "\n",
        "$$ w_{new} = w - m v_{t-1} $$\n",
        "$$ v_t = m v_{t-1} + \\alpha \\nabla_{w_{new}} f $$\n",
        "$$ w = w - v_t $$\n",
        "\n",
        "### Adagrad\n",
        "\n",
        "All previous approaches manipulated the learning rate globally and equally for all parameters. Tuning the learning rates is an expensive process, so much work has gone into devising methods that can adaptively tune the learning rates, and even do so per parameter. \n",
        "\n",
        "Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.\n",
        "\n",
        "$$ c = c + (\\nabla_w f)^2 $$\n",
        "$$ w = w - \\frac{\\alpha}{\\sqrt{c}} $$ \n",
        "\n",
        "\n",
        "### RMProp\n",
        "\n",
        "RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. In particular, it uses a moving average of squared gradients instead, giving:\n",
        "\n",
        "$$ c = \\beta c + (1 - \\beta)(\\nabla_w f)^2 $$\n",
        "$$ w = w - \\frac{\\alpha}{\\sqrt{c}} $$ \n",
        "\n",
        "where $\\beta$ is a decay rate that controls the size of the moving average.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/g1.gif?raw=1\">\n",
        "\n",
        "(Image credit: Alec Radford) \n",
        "\n",
        "<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/g2.gif?raw=1\">\n",
        "\n",
        "(Image credit: Alec Radford) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHA6FD6uIgBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reset\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets.samples_generator import make_regression \n",
        "from scipy import stats \n",
        "import random\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr9t1QGxIgBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the function that I'm going to plot\n",
        "def f(x,y):\n",
        "    return x**2 + 5*y**2\n",
        "\n",
        "x = np.arange(-3.0,3.0,0.1)\n",
        "y = np.arange(-3.0,3.0,0.1)\n",
        "X,Y = np.meshgrid(x, y, indexing='ij') # grid of point\n",
        "Z = f(X, Y) # evaluation of the function on the grid\n",
        "\n",
        "plt.pcolor(X, Y, Z, cmap=plt.cm.gist_earth)\n",
        "plt.axis([x.min(), x.max(), y.min(), y.max()])\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.gcf().set_size_inches((6,6))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHl9t64eIgB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_f(x):\n",
        "    return x[0]**2.0 + 5*x[1]**2.0\n",
        "\n",
        "def part_f(x, \n",
        "           f, \n",
        "           i, \n",
        "           h=1e-6):\n",
        "    w1 = [x_j + (h if j==i else 0) for j, x_j in enumerate(x)]\n",
        "    w2 = [x_j - (h if j==i else 0) for j, x_j in enumerate(x)]\n",
        "    return (f(w1) - f(w2))/(2*h)\n",
        "\n",
        "def gradient_f(x, \n",
        "               f, \n",
        "               h=1e-6):\n",
        "    return np.array([round(part_f(x,f,i,h), 10) for i,_ in enumerate(x)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATGfFhiuIgB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SGD(target_f, \n",
        "        gradient_f, \n",
        "        x, \n",
        "        alpha_0=0.01,\n",
        "        toler = 0.000001):\n",
        "    alpha = alpha_0\n",
        "    min_val = float('inf')\n",
        "    steps = 0\n",
        "    iteration_no_increase = 0\n",
        "    trace = []\n",
        "    while iteration_no_increase < 100:\n",
        "        val = target_f(x)\n",
        "        if min_val - val > toler:\n",
        "            min_val = val\n",
        "            alpha = alpha_0\n",
        "            iteration_no_increase = 0\n",
        "        else:\n",
        "            alpha *= 0.95\n",
        "            iteration_no_increase += 1\n",
        "        trace.append(x)\n",
        "        gradient_i = gradient_f(x, target_f)\n",
        "        x = x - (alpha *  gradient_i)\n",
        "        steps += 1\n",
        "    return x, val, steps, trace\n",
        "\n",
        "x = np.array([2,-2])\n",
        "x, val, steps, trace = SGD(target_f, gradient_f, x)\n",
        "print(x)\n",
        "print('Val: {:.6f}, steps: {:.0f}'.format(val, steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzGDYkuYIgCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SGD_M(target_f, \n",
        "        gradient_f, \n",
        "        x, \n",
        "        alpha_0=0.01,\n",
        "        toler = 0.000001,\n",
        "        m = 0.9):\n",
        "    \n",
        "    alpha = alpha_0\n",
        "    min_val = float('inf')\n",
        "    steps = 0\n",
        "    iteration_no_increase = 0\n",
        "    v = 0.0\n",
        "    trace = []\n",
        "    while iteration_no_increase < 100:\n",
        "        val = target_f(x)\n",
        "        if min_val - val > toler:\n",
        "            min_val = val\n",
        "            alpha = alpha_0\n",
        "            iteration_no_increase = 0\n",
        "        else:\n",
        "            alpha *= 0.95\n",
        "            iteration_no_increase += 1\n",
        "        trace.append(x)\n",
        "        gradient_i = gradient_f(x, target_f)\n",
        "        v = m * v + (alpha *  gradient_i)\n",
        "        x = x - v\n",
        "        steps += 1\n",
        "    return x, val, steps, trace\n",
        "\n",
        "x = np.array([2,-2])\n",
        "x, val, steps, trace2 = SGD_M(target_f, gradient_f, x)\n",
        "\n",
        "print('\\n',x)\n",
        "print('Val: {:.6f}, steps: {:.0f}'.format(val, steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0BDW7CqIgCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x2 = np.array(range(len(trace)))\n",
        "x3 = np.array(range(len(trace2)))\n",
        "plt.xlim([0,len(trace)])\n",
        "plt.gcf().set_size_inches((10,3))\n",
        "plt.plot(x3, trace2)\n",
        "plt.plot(x2, trace, '-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amZwnEjaBcOD",
        "colab_type": "text"
      },
      "source": [
        "http://www.deeplearning.ai/ai-notes/initialization/"
      ]
    }
  ]
}