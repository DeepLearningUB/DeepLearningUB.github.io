{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st Assignment",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2ak-Wm2jQQM"
      },
      "source": [
        "# 1st Assignment: NN's from Scratch\n",
        "\n",
        "The objective is to build from scratch a neural network to perform  classification. Let's firt consider the follwing 2-class dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdHBlrh3xiB"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import matplotlib\n",
        "import autograd.numpy as np\n",
        "from autograd import grad\n",
        "from autograd.misc import flatten\n",
        "\n",
        "# Display plots inline and change default figure size\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (6.0, 4.0)\n",
        "\n",
        "# Generate a dataset and plot it\n",
        "np.random.seed(0)\n",
        "X, y = sklearn.datasets.make_moons(1000, noise=0.20)\n",
        "plt.scatter(X[:,0], X[:,1], s=40, c=y, alpha=0.45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peoKj7dbMa6j"
      },
      "source": [
        "We will use a **3-layer classification neural network** with one input layer, one hidden layer, and one output layer. \n",
        "\n",
        "The number of nodes in the input layer will be determined by the dimensionality of our data (2). Similarly, the number of nodes in the output layer is determined by the number of classes $C$ we have (also 2). \n",
        "\n",
        "Our network makes predictions using forward propagation, which is just a bunch of matrix multiplications and the application of the activation function $\\sigma$. \n",
        "\n",
        "If $x$ is the input to our network then we calculate our prediction $\\hat{y}$ as follows:\n",
        "\n",
        "$$ z_1 = x W_1 + b_1 $$\n",
        "$$ a_1 = \\sigma(z_1) $$\n",
        "$$ z_2 = a_1 W_2 + b_2$$\n",
        "$$ y = \\mbox{softmax}({z_2})$$\n",
        "\n",
        "where $y$ is a multidimensional vector representing a probability distribution over classes, $\\sigma$ is a non linear function and $W_1, b_1, W_2, b_2$ are parameters of our network, which we need to learn from our training data. You can think of the parameters $W$ as matrices transforming data between layers of the network. \n",
        "\n",
        "Looking at the matrix multiplications above we can figure out the dimensionality of these matrices in our problem. If we use 500 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{2\\times500}$, $b_1 \\in \\mathbb{R}^{500}$, $W_2 \\in \\mathbb{R}^{500\\times2}$, $b_2 \\in \\mathbb{R}^{2}$. \n",
        "\n",
        "The last layer is a `softmax` function. The softmax function is a generalization of the logistic function to multiple dimensions. It is used to normalize the output of a network to a probability distribution over predicted output classes.\n",
        "\n",
        "The softmax function takes as input a vector $C$ of real numbers, and normalizes it into a probability distribution consisting of $C$ probabilities proportional to the exponentials of the input numbers:\n",
        "\n",
        "$$softmax(z_i) = \\frac{exp(z_i)}{\\sum_{j=1}^{C}exp(z_j))}$$\n",
        "\n",
        "A common choice with the softmax output is the **cross-entropy loss**. If we have $N$ training examples in our minibatch dataset and $C$ classes, then the loss for our prediction $\\hat{y}$ for that minibatch with respect to the true labels $y$ is given by:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVMz8blaSFey"
      },
      "source": [
        "Let's now define some general parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOYYkyUiSCGF"
      },
      "source": [
        "num_examples = len(X) # training set size\n",
        "nn_input_dim = 2 # input layer dimensionality\n",
        "nn_output_dim = 2 # output layer dimensionality\n",
        "sigma = np.tanh # activation function\n",
        "\n",
        "# Gradient descent parameters \n",
        "epsilon = 0.01 # learning rate for gradient descent\n",
        "reg_lambda = 0.01 # regularization strength\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jc1dMebL43T"
      },
      "source": [
        "# loss function for a 3-layer MLP\n",
        "def loss(model):\n",
        "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
        "    # Forward propagation to calculate our predictions\n",
        "    z1 = np.dot(X,W1) + b1\n",
        "    a1 = sigma(z1)\n",
        "    z2 = np.dot(a1,W2) + b2\n",
        "    exp_scores = np.exp(z2)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    # Calculating the loss\n",
        "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
        "    data_loss = np.sum(corect_logprobs)\n",
        "    # Add regulatization term to loss (optional)\n",
        "    data_loss += reg_lambda * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
        "    return 1./num_examples * data_loss\n",
        "\n",
        "# forward propagation\n",
        "def predict(model, x):\n",
        "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
        "    z1 = np.dot(x,W1) + b1\n",
        "    a1 = sigma(z1)\n",
        "    z2 = np.dot(a1,W2) + b2\n",
        "    exp_scores = np.exp(z2)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    return np.argmax(probs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF9NBSVYMmGV"
      },
      "source": [
        "# This function learns parameters for the neural network and returns the model.\n",
        "# - nn_hdim: Number of nodes in the hidden layer\n",
        "# - num_passes: Number of passes through the training data for gradient descent\n",
        "# - print_loss: If True, print the loss every 1000 iterations\n",
        "\n",
        "def build_model(nn_hdim, num_passes=50000, print_loss=False):\n",
        "    \n",
        "    # Initialize the parameters to random values. \n",
        "    np.random.seed(0)\n",
        "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
        "    b2 = np.zeros((1, nn_output_dim))\n",
        "\n",
        "    # This is what we return at the end\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "    \n",
        "    # Gradient descent. For each batch...\n",
        "    for i in range(0, num_passes):\n",
        "\n",
        "        # Forward propagation\n",
        "        predict(model, X)\n",
        "\n",
        "        # computing the derivative by AD        \n",
        "        gradient_loss = grad(loss)\n",
        "\n",
        "        # flattening nested containers containing numpy arrays\n",
        "        # Returns 1D numpy array and an unflatten function.\n",
        "        model_flat, unflatten_m = flatten(model)\n",
        "        grad_flat, unflatten_g = flatten(gradient_loss(model))\n",
        "        \n",
        "        # gradient descend\n",
        "        model_flat -= grad_flat * epsilon\n",
        "        model = unflatten_m(model_flat)\n",
        "\n",
        "        # Optionally print the loss.\n",
        "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
        "        if print_loss and i % 1000 == 0:\n",
        "            print(\"Loss after iteration %i: %f\" %(i, loss(model)))\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build a model with a 3-dimensional hidden layer\n",
        "model = build_model(3, print_loss=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh067UblQZPv"
      },
      "source": [
        "def plot_decision_boundary(pred_func):\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole gid\n",
        "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, alpha=0.45)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.45)\n",
        "\n",
        "    \n",
        "# Plot the decision boundary\n",
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq_zcStlkZnf"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Add an additional layer to your neural network model. Change the activation function and use a ReLU. Does it work?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjDYawPWMtsy"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jaKG_qzkheu"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "Design (change parameters) and train a NN to classify the following dataset. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM35af3JlVQz"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def twospirals(n_points, noise=.5):\n",
        "    \"\"\"\n",
        "     Returns the two spirals dataset.\n",
        "    \"\"\"\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * 780 * (2*np.pi)/360\n",
        "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
        "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
        "            np.hstack((np.zeros(n_points),np.ones(n_points))))\n",
        "\n",
        "X, y = twospirals(1000)\n",
        "\n",
        "plt.title('training set')\n",
        "plt.plot(X[y==0,0], X[y==0,1], '.', label='class 1')\n",
        "plt.plot(X[y==1,0], X[y==1,1], '.', label='class 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7owY4wXFkjnX"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}